{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "c24627ff5a866b1070e9fc50f5b6a487",
     "grade": false,
     "grade_id": "cell-12bea12324c032d8",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import hashlib # for grading\n",
    "\n",
    "# Standard imports\n",
    "import numpy as np\n",
    "from numpy import inf\n",
    "import pandas as pd\n",
    "from collections import Counter, OrderedDict\n",
    "import re\n",
    "import string\n",
    "import math\n",
    "from collections import Counter\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "\n",
    "# NLTK imports\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# SKLearn related imports\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('data/imdb_sentiment.csv')\n",
    "\n",
    "# Get the text\n",
    "docs = df['text']\n",
    "\n",
    "# Split in train and validation\n",
    "train_df, validation_df = train_test_split(df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "665c586b89d011523f4f5920f8e982cd",
     "grade": false,
     "grade_id": "cell-f07b8631beb0508c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Q1.\n",
    "\n",
    "You were a given the task of answering some questions about a file with the 10000 most commons password from the latest leak your rival corporation has suffered. You don't want to suffer the same fate, so you are trying to come up with a set of restrictions that should reduce the chances of this happening on your side!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "d718ab3ffc09e8d3ce8c98db4e7fcfcf",
     "grade": false,
     "grade_id": "cell-a88528290dd35f7d",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def find_all_in_file(pattern, path):\n",
    "    \"\"\"\n",
    "    Function that returns all matches of a certain pattern in a certain text.\n",
    "    Args:\n",
    "    pattern - regex pattern\n",
    "    path - path to the file\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    return re.findall(pattern,\n",
    "                      open(path, 'r').read(),\n",
    "                      re.MULTILINE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "841f827d503ce76269f27c7cc3503f37",
     "grade": false,
     "grade_id": "cell-df341d1f1838c29e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### Q1.a)\n",
    "\n",
    "Everybody knows that long password tend to be more secure.. Your boss wonders how many of those common password were even 10 or more characters. Get him the number of passwords that have 10 or more characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "466dba8f110bf4b782aaad136cf065c2",
     "grade": false,
     "grade_id": "cell-7cddd8e2e48afb31",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# ret = find_all_in_file(...)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "ret = find_all_in_file(\"^[^\\s]{10,}$\", \n",
    "                       \"data/10k-most-common.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "0f8ffbfd2b759498ec8e88addcd8907e",
     "grade": true,
     "grade_id": "cell-5a358ed83c473214",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of password with more than 10 or more characters:  51\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of password with more than 10 or more characters: \", len(ret))\n",
    "assert len(ret) == 51"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "f293dbb892b7711fa9bf1fbf87fea18b",
     "grade": false,
     "grade_id": "cell-6727f8213243afc0",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### Q1.b)\n",
    "\n",
    "As expected people are lazy and have short passwords. What about the number of passwords that start with digits? And the number of passwords that don't start with digits? \n",
    "\n",
    "(Get the number of passwords that start with digits and without digits.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "fe1cdcdac53faecd06a2d94668afc292",
     "grade": false,
     "grade_id": "cell-447eb675481e47fd",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# ret = find_all_in_file(...)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "ret = find_all_in_file(\"^[0-9].+$\", \"data/10k-most-common.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "c6e84f819e447119ba39e2f5f1d7e8df",
     "grade": true,
     "grade_id": "cell-535072609d84d577",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of passwords that start with digits:  659\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of passwords that start with digits: \" , len(ret))\n",
    "assert len(ret) == 659"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "c3857a0456bdc834cc9629b3f5bc64d1",
     "grade": false,
     "grade_id": "cell-66af8912995d91a2",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# ret = find_all_in_file(...)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "ret = find_all_in_file(\"^[^0-9].+$\", \"data/10k-most-common.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "3ff5be546cce6fab4e24e5756ecc4a5b",
     "grade": true,
     "grade_id": "cell-1c17b943b63b52ae",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of passwords that start without digits:  9341\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of passwords that start without digits: \" , len(ret))\n",
    "assert len(ret) == 9341"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "15f2d492882f16ccb4fee2b258fcfac1",
     "grade": false,
     "grade_id": "cell-4e3cc736b933d71e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### Q1.c)\n",
    "\n",
    "So, people prefer to start their passwords with other things than digits. That was to be expected too. Your boss is also very curious about the number of passwords that are variations of the word _pass_. Can you get him that number?\n",
    "\n",
    "(Get the number of passwords that are variants of the word pass. For instance _pass_, _password_, _passpass_, should be matched.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "fbf77f1e78ffca004570009c5ba4a4fe",
     "grade": false,
     "grade_id": "cell-2bf14a2df3dd3e4e",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# ret = find_all_in_file(...)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "ret = find_all_in_file(\"^pass[A-Za-z]*$\", \"data/10k-most-common.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "52ddbbe171f13ae39bde6a4467762e65",
     "grade": true,
     "grade_id": "cell-19b61469674be299",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of variants of \"pass\":  13\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of variants of \\\"pass\\\": \" , len(ret))\n",
    "assert len(ret) == 13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "e7e858cc917e76bd95acaf4d56893cde",
     "grade": false,
     "grade_id": "cell-f6bb573c7736db8e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### Q1.d)\n",
    "\n",
    "Your boss now comes up with the idea that a password should start with 3 numbers and be followed by letters, but only from b to m. You argue that this would make the task of guessing the password even easier. But he doesn't care. The only way to get him out of this idea is to show him that some passwords in this list follows that pattern. Can you find those passwords?\n",
    "\n",
    "(Get the number of password that start with 3 numbers and that end with any number of letters from b to m.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "55f0954be2ddae5fb260e4b5f9270d5d",
     "grade": false,
     "grade_id": "cell-e92132a5b63400f3",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# ret = find_all_in_file(...)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "ret = find_all_in_file(\"^[0-9]{3}[b-m]+$\", \"data/10k-most-common.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "88cde3afb32f7257f2ed9fe03da557ee",
     "grade": true,
     "grade_id": "cell-5150c254b592778e",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of password:  2\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of password: \" , len(ret))\n",
    "assert len(ret) == 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "a2a60821c686f86449c39e7383b53d11",
     "grade": false,
     "grade_id": "cell-b8d6fa2eb4f62760",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "651beb568cdc9a4eebec2683524ff017",
     "grade": false,
     "grade_id": "cell-0409818769bbf3d0",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Q2.\n",
    "\n",
    "For this question we will be looking at the first few pages of a [great book](https://en.wikipedia.org/wiki/1Q84). You will be ask to perform some common pre-processing operations in natural language processing so you can more easily manipulate the data to asnswer the next questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "2ab28c5fd588ffab16e8c8349022fe76",
     "grade": false,
     "grade_id": "cell-addc0c904c359402",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### Q2.a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "e7beab45bdd82f6a239b0bf8eefa3375",
     "grade": false,
     "grade_id": "cell-fdcd8b7e92e5e588",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "The first thing you will have to do is to tokenize the data. We want to get the words of the provided sample of \"1Q84\", so, choose the tokenizer accordingly. (Import it from nltk)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "92d4409c09d24be91697c7721c8f2a38",
     "grade": false,
     "grade_id": "cell-a8b7930a61806e32",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def tokenize_all_file(tokenizer, file):\n",
    "    \"\"\"\n",
    "    Returns the a list with the tokens of given text\n",
    "    \n",
    "    Args:\n",
    "    tokenizer - nltk tokenizer\n",
    "    file - path to the file to tokenize\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize the list\n",
    "    # text = ...\n",
    "    # Tokenize each line of the file\n",
    "    # ...\n",
    "    # Flatten the list\n",
    "    # text_flat = ...\n",
    "    # return text_flat\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    text = list()\n",
    "    \n",
    "    for line in open(file, \"r\"):\n",
    "        text.append(tokenizer.tokenize(line))\n",
    "    \n",
    "    text_flat = [w for s in text for w in s]\n",
    "\n",
    "    return text_flat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "95c989af964786738776ddc49b043962",
     "grade": false,
     "grade_id": "cell-864f368a29fd9c6b",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# tokenizer = ...\n",
    "# path = \"../data/murakami.txt\"\n",
    "# text = tokenize_all_file(...)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "tokenizer = WordPunctTokenizer()\n",
    "path = \"data/murakami.txt\"\n",
    "text = tokenize_all_file(tokenizer, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "136fe17d0f7864486788dee5f7aecbc0",
     "grade": true,
     "grade_id": "cell-f829c5a222c54690",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert len(text) == 2582"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "107037420ff4aff8bd4be3b6b9578d2f",
     "grade": false,
     "grade_id": "cell-c8044c14c20583cf",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### Q2.b)\n",
    "\n",
    "Now that the data is tokenized you want to answer this question: \"How many different non-stopwords words did Murakami use in the beginning pages?\" \n",
    "\n",
    "NOTE: Stopwords adapted from [here](Stopwords adapted from [here](https://gist.github.com/sebleier/554280). (Notice what we added some specific things, like ?\" and .\" to the stopwords. This was shown to be a limitation of the nltk tokenizer so it will be removed that way, instead of the more conventional way. This goes to show that there are more powerful tokenizers that you should use in the case you have to perform tokenization in the future.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "98b29d5ad5fe4b5aac109b23df7dd59c",
     "grade": false,
     "grade_id": "cell-8805d7a045441a7c",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_data(tokens, sw_path, filter_sw=False):\n",
    "    \"\"\"\n",
    "    Returns list with the preprocessed data\n",
    "    Args:\n",
    "    tokens - tokenized list\n",
    "    sw_path - path to the subwords\n",
    "    filter_sw - set to true if you want to remove subwords\n",
    "    \"\"\"\n",
    "    \n",
    "    #if filter_sw:\n",
    "        # Create the list of stopwords from the file english_stopwords.txt\n",
    "        # stopwords = ...\n",
    "        # Lowercase all the words and make \n",
    "        # lc_text = ...\n",
    "        # Filter the stopwords from the text\n",
    "        # rem_sw_text = ...\n",
    "        # Remove the punctuation (take advantage of string.punctuation)\n",
    "        # filtered_text = ...\n",
    "    \n",
    "    #else:\n",
    "        # Lowercase all the words and make \n",
    "        # lc_text = ...\n",
    "        # Remove the punctuation (take advantage of string.punctuation)\n",
    "        # filtered_text = ...\n",
    "    \n",
    "    #return filtered_text\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    if filter_sw:\n",
    "        # Create the list of stopwords from the file english_stopwords.txt\n",
    "        stopwords = [line.strip(\"\\n\") for line in open(sw_path, \"r\")]\n",
    "        # Lowercase all the words and make \n",
    "        lc_text = [w.lower() for w in tokens]\n",
    "        # Filter the stopwords from the text\n",
    "        rem_sw_text = list(filter(lambda x: x not in stopwords, lc_text))\n",
    "        # Remove the punctuation (take advantage of string.punctuation)\n",
    "        filtered_text = list(filter(lambda x: x not in string.punctuation, rem_sw_text))\n",
    "    \n",
    "    else:\n",
    "        # Lowercase all the words and make \n",
    "        lc_text = [w.lower() for w in tokens]\n",
    "        # Remove the punctuation (take advantage of string.punctuation)\n",
    "        filtered_text = list(filter(lambda x: x not in string.punctuation, lc_text))\n",
    "    \n",
    "    return filtered_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "f26bbde392e94a51cd548f47f877491a",
     "grade": false,
     "grade_id": "cell-5c7c5a8d817108b7",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Get the filtered tokens list\n",
    "# filtered_text = preprocess_data(...)\n",
    "# Number of different words\n",
    "# nr_diff_words = ,,,\n",
    "\n",
    "# YOUR CODE HERE\n",
    "filtered_text = preprocess_data(text, \"data/english_stopwords.txt\", filter_sw=True)    \n",
    "# Number of different words\n",
    "nr_diff_words = len(set(filtered_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "c5c01202539f0a2813f2ead12083c77c",
     "grade": true,
     "grade_id": "cell-f94749703218e9af",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Murakami used 687 different words (stopwords not included).\n"
     ]
    }
   ],
   "source": [
    "print(\"Murakami used {} different words (stopwords not included).\".format(nr_diff_words))\n",
    "assert len(filtered_text) == 1119\n",
    "assert nr_diff_words == 687"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "c983de607fcb45bee1bd1175e56e91a2",
     "grade": false,
     "grade_id": "cell-faa09f2ad9d14d4a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "77f65eb0f486b8f151323cac4a79b8cc",
     "grade": false,
     "grade_id": "cell-508c27436887186e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Q3.\n",
    "\n",
    "Your goal is now to find the top three most common word stems. You should apply the snowball stemmer at the `filtered_text` you obtained in Q2. Take advantage of the function `Counter` from [collections](https://docs.python.org/2/library/collections.html) to find the six most common stems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "07db482ca654ebae4a16a5f9d75f892e",
     "grade": false,
     "grade_id": "cell-98884174107979f5",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def stem_list(tokens):\n",
    "    \"\"\"\n",
    "    Returns a list with the stems of a list of tokens\n",
    "    Args:\n",
    "    tokens - list with the tokens\n",
    "    \"\"\"\n",
    "    #stemmer = SnowballStemmer(...)\n",
    "    #stems = ...\n",
    "    #return stems\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "    stems = list(map(stemmer.stem, tokens))\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "de6091777a0b5bbf5ec754330fc93b07",
     "grade": false,
     "grade_id": "cell-43e8c6c7722e2b0b",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('aomam', 28),\n",
       " ('driver', 18),\n",
       " ('like', 13),\n",
       " ('name', 13),\n",
       " ('could', 11),\n",
       " ('would', 11)]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#stems = stem_list(...)\n",
    "#Call counter to get the most common 6 stems\n",
    "\n",
    "# YOUR CODE HERE\n",
    "stems = stem_list(filtered_text)\n",
    "Counter(stems).most_common(6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "f038ec5b551525d4b80858a8302c02a6",
     "grade": false,
     "grade_id": "cell-7cb834f08de238d9",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# answer = ... (copy the output of the previous cell. It should be a list with 6 tuples)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "answer = [('aomam', 28), ('driver', 18), ('name', 13), ('like', 13), ('could', 11), ('would', 11)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "130c379163c196fc68dd1c4c06a815de",
     "grade": true,
     "grade_id": "cell-95a3f0176d152730",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert answer == [('aomam', 28), ('driver', 18), ('name', 13),\n",
    "                  ('like', 13), ('could', 11), ('would', 11)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "453f532094f4f0f15f0d6f3a5ee7bcc9",
     "grade": false,
     "grade_id": "cell-b03a5b848243133d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "ddf7145ba4f89c292fbde101e8e67fc4",
     "grade": false,
     "grade_id": "cell-f48abbdab8acc3d1",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Q4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "33758322965927e40dd2a976be24c165",
     "grade": false,
     "grade_id": "cell-d6a39b05ecd310c0",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### Q4.a)\n",
    "\n",
    "Start by implementing your own function to get the list of n-grams from a list of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "0c96975d4a1ca4a067781b7681aa1ea1",
     "grade": false,
     "grade_id": "cell-43601c5f562f5867",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def ngrams(n, text):\n",
    "    \"\"\"\n",
    "    Returns list of tuples for all the n-grams\n",
    "    \n",
    "    Args:\n",
    "    n - the n in n-grams\n",
    "    text: list of tokenized data\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    return [tuple(text[i:i+n]) for i in range(len(text)-n+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "c70e8d422ae51c1e544796821ef3c202",
     "grade": true,
     "grade_id": "cell-f601c9c7d4b35092",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert ngrams(2, \"The driver made a mistake\".split()) == [('The', 'driver'), ('driver', 'made'), ('made', 'a'), ('a', 'mistake')]\n",
    "assert ngrams(3, \"The driver made a mistake\".split()) == [('The', 'driver', 'made'), ('driver', 'made', 'a'), ('made', 'a', 'mistake')]\n",
    "assert ngrams(4, \"The driver made a mistake\".split()) == [('The', 'driver', 'made', 'a'), ('driver', 'made', 'a', 'mistake')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "1ff6d2ff58e8a89c8a777a0d4efa1188",
     "grade": false,
     "grade_id": "cell-8cdc8657fcd5f3c4",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### Q4.b)\n",
    "\n",
    "Now return to Murakami's text and, using preprocessed data without removing the stopwords, find the most common bigram, trigram and 4-gram. Once again, take advantage of `Counter` and `most_common()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "4aabea48cb307a270f52ac6840e82bd5",
     "grade": false,
     "grade_id": "cell-539dba0b0aabb366",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# filtered_text = preprocess_data(...)\n",
    "# bigram_ = ...\n",
    "# trigram_ = ...\n",
    "# fourgram_ = ...\n",
    "\n",
    "# YOUR CODE HERE\n",
    "filtered_text = preprocess_data(text, \"\")\n",
    "bigram_ = Counter(ngrams(2, filtered_text)).most_common(1)[0]\n",
    "trigram_ = Counter(ngrams(3, filtered_text)).most_common(1)[0]\n",
    "fourgram_ = Counter(ngrams(4, filtered_text)).most_common(1)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "b1e99d28845c1787d1caa943d5ab0db2",
     "grade": true,
     "grade_id": "cell-91094315aa4e5abf",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert bigram_ == (('the', 'driver'), 15)\n",
    "assert trigram_ == ((',\"', 'the', 'driver'), 6)\n",
    "assert fourgram_ == ((',\"', 'the', 'driver', 'said'), 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "4393d847f0b93102bc53c2e577f73ea4",
     "grade": false,
     "grade_id": "cell-f92262bde6007da8",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "What is the highest value of n, before the most common n-gram starts to be unique?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "57bd7afcbe3605a48ac5762ba2b32236",
     "grade": false,
     "grade_id": "cell-566e26fa1479e9ff",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# n = ...\n",
    "\n",
    "# YOUR CODE HERE\n",
    "for n in range(1, 50):\n",
    "    ans = Counter(ngrams(n, filtered_text)).most_common(1)[0][1]\n",
    "    if ans > 1: continue\n",
    "    else: n-=1; break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "ae1c32e95ffb18168a17e61c4aa5d8a3",
     "grade": true,
     "grade_id": "cell-a060844c68988375",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "answer = hashlib.sha256(bytes(n)).hexdigest()\n",
    "\n",
    "assert answer == \"837885c8f8091aeaeb9ec3c3f85a6ff470a415e610b8ba3e49f9b33c9cf9d619\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "669a5ae914346ba251b385bf530730bd",
     "grade": false,
     "grade_id": "cell-b36182b652e26ee5",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Q5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "095e480222cee049a9d852fa2c30dafe",
     "grade": false,
     "grade_id": "cell-081bbba1b14140e2",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Which of these statements is true?\n",
    "\n",
    "**A** - Text is structured and so can be immediately used by a classifier, without much preprocessing needed.\n",
    "\n",
    "**B** - A Bag of Words representation carries syntactic information (i.e. word order matters in the BoW representation).\n",
    "\n",
    "**C** - Removal of stopwords may or may not be useful to a certain usecase, depending on our choice of features and dataset used.\n",
    "\n",
    "**D** - The less a word appears in a document and the more it appears in other documents, the higher is the TF-IDF of that word in that document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "9226aae5292f5677e4fb30dd3dc12f9d",
     "grade": false,
     "grade_id": "cell-24849fb80014cefb",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "#answer = # Answer with a one character string, capitalized\n",
    "\n",
    "\n",
    "# YOUR CODE HERE\n",
    "answer = 'C'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "79841ee2106878411b856a0e345f18ff",
     "grade": true,
     "grade_id": "cell-2ce91f1c34af8032",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "expected_hash = '6b23c0d5f35d1b11f9b683f0b0a617355deb11277d91ae091d399c655b87940d'\n",
    "assert hashlib.sha256(str(answer).encode()).hexdigest() == expected_hash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "d104971bd140532edbbcfbba1b538e65",
     "grade": false,
     "grade_id": "cell-a5d7f504c1659b98",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Q6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "fb3ec0d5f993ff920efdff7f82da7bb7",
     "grade": false,
     "grade_id": "cell-d2477efb5c417c3d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Let's get once again a small sample of our dataset and manually vectorize and transform it into a BoW representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "e7dd46a930113189e7faf1609d07c2a2",
     "grade": false,
     "grade_id": "cell-23a2f9da897ab002",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "small_docs = docs[:200]\n",
    "stopwords = [line.strip(\"\\n\") for line in open(\"data/english_stopwords.txt\", \"r\")]\n",
    "\n",
    "def build_vocabulary():\n",
    "    vocabulary = Counter()\n",
    "\n",
    "    for doc in small_docs:\n",
    "        words = [word for word in doc.split() if word not in stopwords]\n",
    "        vocabulary.update(words)\n",
    "    \n",
    "    return OrderedDict(vocabulary.most_common())\n",
    "\n",
    "def vectorize():\n",
    "    vocabulary = build_vocabulary()\n",
    "    vectors = []\n",
    "    for doc in small_docs:\n",
    "        words = doc.split()\n",
    "        vector = np.array([doc.count(word) for word in vocabulary if word not in stopwords])\n",
    "        vectors.append(vector)\n",
    "    \n",
    "    return vectors\n",
    "\n",
    "def build_df():\n",
    "    return pd.DataFrame(vectorize(), columns=build_vocabulary())\n",
    "\n",
    "BoW = build_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "b0bf79b5dda750ef4578c5d2941d5e2a",
     "grade": false,
     "grade_id": "cell-c9951e7e69d2e1c9",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Your boss loves TF-IDF so he asks you to implement it. However, he prefers a slightly different formulation from the one we learned before:\n",
    "\n",
    "$$ tfidf _{t, d} =(tf_{t,d})*(log_2{(1 + \\frac{N}{df_{t}})})  $$\n",
    "\n",
    "Implement his preferred version of TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "b17d7bfc7f3796ba03267676b60e070a",
     "grade": false,
     "grade_id": "cell-eb6996cb41762895",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def new_tfidf(BoW_df):\n",
    "    \"\"\"\n",
    "    Returns pandas dataframe of a tfidf representation from a BoW representation dataframe.\n",
    "\n",
    "    Args:\n",
    "    BoW_df - dataframe with document word counts (Bag of Words)\n",
    "    \"\"\"\n",
    "    # tf = (...)\n",
    "    \n",
    "    # def _idf(column):\n",
    "    #   return (...)\n",
    "    \n",
    "    # tf_idf = (...)\n",
    "    \n",
    "    # return tf_idf\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    tf = BoW.div(BoW.sum(axis=1), axis=0)\n",
    "\n",
    "    def _idf(column):\n",
    "        return np.log2(1 + len(column) / sum(column > 0))\n",
    "\n",
    "    tf_idf = (tf).multiply(tf.apply(_idf))\n",
    "    \n",
    "    return tf_idf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "2088e522d9fed36a57c93eff66e659ca",
     "grade": false,
     "grade_id": "cell-7c0f1feaa6b613fb",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "new_scores = new_tfidf(BoW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "11486395f13fa16eaa3b6d7b694476c2",
     "grade": true,
     "grade_id": "cell-5b8f988a69678772",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert(math.isclose(new_scores['movie'][0], 0.009717385023827248),\n",
    "       math.isclose(new_scores['film'][10], 0.019778475747522496),\n",
    "       math.isclose(new_scores['nice'][16], 0.010851136310680626),\n",
    "       math.isclose(new_scores['good'][128], 0.00989061193998239))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "c357348df036fb293ea30ad443aa2100",
     "grade": false,
     "grade_id": "cell-07457d37962bd876",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Q7."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "e4fb075631e7d7d02a88407fd1d66de2",
     "grade": false,
     "grade_id": "cell-779badbcfbbc934e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Let's get our `TextCleanerTransformer` and clean our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "0b8a5ac8de2d411a5d67bf6a53cb370d",
     "grade": false,
     "grade_id": "cell-8300a0a7bebf4efd",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "from TextCleanerTransformer import TextCleanerTransformer\n",
    "# Initialize a tokenizer and a stemmer\n",
    "tokenizer = WordPunctTokenizer()\n",
    "stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "regex_list = [(\"<[^>]*>\", \"\")\n",
    "             ]\n",
    "cleaner = TextCleanerTransformer(tokenizer, stemmer, regex_list)\n",
    "docs = cleaner.transform(df.text.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "b752ea7f2761b7394a2d1dd9cf02102e",
     "grade": false,
     "grade_id": "cell-2b548aa7912824b9",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Here's one of the movie reviews in the imdb dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "b18c3a1c86d80a7a85db5c3fc716cc58",
     "grade": false,
     "grade_id": "cell-48dc96b64babda47",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "df['text'].values[2013]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "abba1dca6a4710b56cecace86dc2cb12",
     "grade": false,
     "grade_id": "cell-842b4dd41f5c7360",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "For this exercise, transform your documents into a matrix of tf-idf scores using sklearn. Then, get the corresponding string of the most important word of this document (with index `2013`) according to TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "497414114c9205fb5a5784a7f3ddddf1",
     "grade": false,
     "grade_id": "cell-a202493c86db26ac",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(docs)\n",
    "word_count_matrix = vectorizer.transform(docs)\n",
    "tfidf = TfidfTransformer()\n",
    "tfidf.fit(word_count_matrix)\n",
    "word_term_frequency_matrix = tfidf.transform(word_count_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "e47b9135ab6ef816789a44ce5cde5355",
     "grade": false,
     "grade_id": "cell-bcc7341a8f2e3160",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def most_relevant_word(word_term_frequency_matrix, idx):\n",
    "    \"\"\"\n",
    "    Returns the word with highest tfidf score in a specified document.\n",
    "    \n",
    "    Args:\n",
    "    word_term_frequency_matrix - tfidf sparse matrix of the dataset\n",
    "    idx - index of the relevant document\n",
    "    \"\"\"\n",
    "    # max_word_idx = (...)\n",
    "    # vocab = (...)\n",
    "    \n",
    "    # most_relevant_word = (...)\n",
    "    \n",
    "    # return most_relevant_word\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    max_word_idx = word_term_frequency_matrix[idx].argmax()\n",
    "    vocab = vectorizer.vocabulary_\n",
    "    inv_vocab = {v: k for k, v in vocab.items()}\n",
    "    most_relevant_word = inv_vocab[max_word_idx]\n",
    "    \n",
    "    return most_relevant_word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "25c20756f714f67a8850b6cb9ecef549",
     "grade": true,
     "grade_id": "cell-962273848ec48118",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert(most_relevant_word(word_term_frequency_matrix, 2013) == 'morena')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "a3424f3372ac141704769a5a94da0e24",
     "grade": false,
     "grade_id": "cell-dec6e2c834643245",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Q8."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "d4f8707629bd95a43e2cdd3fd7e5b7c8",
     "grade": false,
     "grade_id": "cell-ab54cc6297c36ee2",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "In this exercise, build a Pipeline to classify a review as positive or negative. Use `MultinomialNB` as your final classifier, train it and get an accuracy score above 86% on the imdb validation dataset, by choosing the best set of parameters of `TextCleanerTransformer()`, `CountVectorizer()` and `TfidfTransformer()`, according to what we learned in Part III.\n",
    "\n",
    "Hint: Try to use more than unigrams! Also, remember what we said about stopwords and feature space size in Part III of the Learning Notebooks?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "b0a332edbc074dc2d381e270da030031",
     "grade": false,
     "grade_id": "cell-ca9cc0a788f3d721",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Encode the labels\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(train_df['sentiment'].values)\n",
    "\n",
    "train_df['sentiment'] = le.transform(train_df['sentiment'].values)\n",
    "validation_df['sentiment'] = le.transform(validation_df['sentiment'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "eddc5f676907bd18590192257a92e41f",
     "grade": false,
     "grade_id": "cell-e0712ceb628e6ed6",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def train_and_validate():\n",
    "    \"\"\"\n",
    "    Train a model using sklearn's Pipeline and return it along with its \n",
    "    current accuracy in the validation set.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Build the pipeline\n",
    "    # text_clf = Pipeline(...)\n",
    "    \n",
    "    # Train the classifier\n",
    "    # (...)\n",
    "\n",
    "    # predicted = (...)\n",
    "    # acc = (...)\n",
    "    \n",
    "    # return text_clf, acc\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    text_clf = Pipeline([('stemm', TextCleanerTransformer(tokenizer, stemmer, regex_list)),\n",
    "                       ('vect', CountVectorizer(ngram_range=(1,2), max_features=30000)),\n",
    "                       ('tfidf', TfidfTransformer()),\n",
    "                       ('clf', MultinomialNB())])\n",
    "\n",
    "    # Train the classifier\n",
    "    text_clf.fit(map(str, train_df['text'].values), train_df['sentiment'].values)\n",
    "\n",
    "    predicted = text_clf.predict(map(str, validation_df['text'].values))\n",
    "    acc = np.mean(predicted == validation_df['sentiment'])\n",
    "\n",
    "    return text_clf, acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "7aa04e7bb141e84cc710eaf8f3186779",
     "grade": true,
     "grade_id": "cell-07b6f3694941ac81",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.869\n"
     ]
    }
   ],
   "source": [
    "_, acc = train_and_validate()\n",
    "print(\"Accuracy: {}\".format(acc))\n",
    "assert(acc >= 0.86)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
